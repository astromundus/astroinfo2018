{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### From http://www.astroml.org/gatspy/:\n",
    "\n",
    "\n",
    "# gatspy: General tools for Astronomical Time Series in Python\n",
    "\n",
    "### Gatspy contains efficient, well-documented implementations of several common routines for Astronomical time series analysis, including the Lomb-Scargle periodogram, the Supersmoother method, and others.\n",
    "\n",
    "### Gatspy was created by Jake VanderPlas. If you use this package for a publication, please consider citing our [paper](http://adsabs.harvard.edu/abs/2015arXiv150201344V) and the [code](http://dx.doi.org/10.5281/zenodo.14833) .\n",
    "\n",
    "### For issues & contributions, see the source repository on [github](http://github.com/astroML/gatspy/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Time Series Analysis\n",
    "\n",
    "Time series analysis is, in many ways, very similar to regression analysis that we heard about, except that we replace $x$ with $t$.  There, we will generally assume that the $y$ values are independent, whereas for time series $y_{i+1}$ is likely to depend directly on $y_i$.  Furthermore, we make no assumptions about the regularity of the time sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Eyer and Mowlavi (2007, arXiv:0712.3797) \n",
    "<img src=\"figures/EyerMowlavi2007.png\" style=\"float: left; width: 90%; margin-right: 1%;\"> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Simplified:\n",
    "\n",
    "<img src=\"figures/variability-tree.jpg\" style=\"float: left; width: 95%; margin-right: 1%;\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There is a broad range of variability signatures that we want to be aware of. From transient events to periodic variables to stochastic sources.\n",
    "\n",
    "<img src=\"figures/flare.png\" style=\"float: left; width: 30%; margin-right: 1%;\"> <img src=\"figures/cepheid.png\" style=\"float: left; width: 30%; margin-right: 1%;\"> <img src=\"figures/eclipsing.png\" style=\"float: left; width: 30%; margin-right: 1%; margin-bottom: 0.5em;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goals\n",
    "\n",
    "When dealing with time series data, the first thing that we want to know is if the system that we are studying is even variable (otherwise, there is no point doing time series analysis!).  In the context of frequentist statistics, this is a question of whether our data would have been obtained by chance if the no-variability null hypothesis were correct.\n",
    "\n",
    "If we find that our source *is* variable, then our time-series analysis has two main goals:\n",
    "1. Characterize the temporal correlation between different values of $y$ (i.e., characterize the \"light curve\").  For example by learning the parameters for a model.\n",
    "2. Predict future values of $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parameter Estimation, Model Selection, and Classification\n",
    "\n",
    "Time series analysis can be conducted in either the time domain or the frequency domain.  We'll first start with a discussion of the time domain by using tools that we already discussed, such as model parameter estimation.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can fit a model to $N$ data points $(t_i,y_i)$:\n",
    "\n",
    "$$y_i(t_i) = \\sum_{m=1}^M \\beta_m T_m(t_i|\\theta_m) + \\epsilon_i,$$\n",
    "\n",
    "where the functions, $T_m$, do not need to be periodic, $t_i$ does not need to be evenly sampled and $\\theta_m$ are the model parameters.\n",
    "\n",
    "So, for example, if we have\n",
    "\n",
    "$$y_i(t_i) = a \\sin(\\omega_0 t_i) + b \\cos (\\omega_1 t_i),$$\n",
    "\n",
    "then $a=\\beta_0$, $b=\\beta_1$, $\\omega_0=\\theta_0$, and $\\omega_1 = \\theta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Determining whether a variable model is favored over a non-variable model is the same as we have dealt with previously and can also be approached using the AIC, BIC, or Bayesian odds ratio.  Once the model parameters, $\\theta_m$ have been determined, we can further apply supervised or unsupervised classification methods (to be discussed in a few weeks) to gain further insight.\n",
    "\n",
    "Common deterministic models include\n",
    "\n",
    "$$T(t) = \\sin(\\omega t)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$T(t) = \\exp(-\\alpha t),$$\n",
    "\n",
    "where the frequency, $\\omega$, and decay rate, $\\alpha$, are parameters to be estimated from the data.\n",
    "\n",
    "We will also explore a more complicated so-called \"chirp\" signal with\n",
    "\n",
    "$$T(t) = \\sin(\\phi + \\omega t + \\alpha t^2).$$\n",
    "\n",
    "(another way of thinking of a chirp is that the *frequency varies with time*; $\\omega_{\\rm instantaneous} = \\omega + \\alpha t$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fourier Analysis\n",
    "\n",
    "Fourier Analysis has the potential to be the subject for an entire class as opposed to part of a single lecture. \n",
    "\n",
    "The code below demostrates an application that you are surely familiar with: reconstruction of a complicated signal by summation of simple trigonometric functions: \n",
    "\n",
    "$$y_i(t_i) = Y_o + \\sum_{m=1}^M \\beta_m \\sin(m \\omega t_i + \\phi_m)   + \\epsilon_i.$$\n",
    "\n",
    "Note: **any** periodic function can be described within noise with a sufficiently large M! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Periodic signals ($\\S$ 10.3)\n",
    "\n",
    "Many systems have **periodic** signals.  This is especially true in astronomy (e.g., RR-Lyrae, Cepheids, eclipsing binaries).\n",
    "\n",
    "What we want to be able to do is to detect variability and measure the period in the face of both noisy and incomplete data.\n",
    "\n",
    "For example, the figure on the left is the kind of data that you *want* to have, whereas the figure on the right is the kind of data that you are more likely to have.\n",
    "<img src=\"figures/rrlyrae-good.png\" style=\"float: left; width: 40%; margin-right: 1%;\"> <img src=\"figures/rrlyrae-bad.png\" style=\"float: left; width: 40%; margin-right: 1%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For a periodic signal we have:\n",
    "\n",
    "$$y(t+P)=y(t),$$ where $P$ is the period.\n",
    "\n",
    "We can create a *phased light curve* that plots the data as function of phase:\n",
    "\n",
    "$$\\phi=\\frac{t}{P} - {\\rm int}\\left(\\frac{t}{P}\\right),$$\n",
    "\n",
    "where ${\\rm int}(x)$ returns the integer part of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Significance of the peaks in the periodogram\n",
    "\n",
    "The amplitude(s) of the periodic signal can be derived from the posterior in much the same way as we do for MLE, by taking the derivative of the posterior with respect to $a$ and $b$.\n",
    "\n",
    "But what we really want to know is the \"best value\" of $\\omega$! \n",
    "\n",
    "The $\\chi^2$ is given by\n",
    "$$\\chi^2(\\omega) \\equiv {1 \\over {\\sigma^2}} \\sum_{j=1}^N [y_j-y(t_j)]^2 =\n",
    "  {1 \\over {\\sigma^2}} \\sum_{j=1}^N [y_j- a_0\\, \\sin(\\omega t_j) - b_0 \\, \\cos(\\omega t_j)]^2,$$\n",
    "  \n",
    "which we can simplify (see the textbook) to\n",
    "\n",
    "$$\\chi^2(\\omega) =  \\chi_0^2 \\, \\left[1 - {2 \\over N \\, V}  \\, P(\\omega) \\right],$$\n",
    "\n",
    "where, again, $P(\\omega)$ is the periodogram and $\\chi_0^2$ is the $\\chi^2$ for a model with $y(t)$=constant:\n",
    "\n",
    "$$  \\chi_0^2 = {1 \\over {\\sigma^2}} \\sum_{j=1}^N y_j^2 = {{N \\, V} \\over {\\sigma^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll now renormalise the periodogram, defining the [Lomb-Scargle periodogram](https://en.wikipedia.org/wiki/Least-squares_spectral_analysis#The_Lomb.E2.80.93Scargle_periodogram) as\n",
    "\n",
    "$$P_{\\rm LS}(\\omega) = \\frac{2}{N V} P(\\omega),$$\n",
    "\n",
    "One can show that $0 \\le P_{\\rm LS}(\\omega) \\le 1$.\n",
    "\n",
    "With this renormalization, the reduction in $\\chi^2(\\omega)$ for the harmonic model, \n",
    "relative to $\\chi^2$ for the pure noise model, $\\chi^2_0$ is\n",
    "$${\\chi^2(\\omega) \\over \\chi^2_0}=  1 - P_{LS}(\\omega).$$\n",
    "\n",
    "\n",
    "To determine if our source is variable or not, we first compute $P_{\\rm LS}(\\omega)$ and then model the odds ratio for our variability model vs. a no-variability model.\n",
    "\n",
    "If our variability model is \"correct\", then the peak of $P(\\omega)$ [found by grid search] gives the best $\\omega$ and the $\\chi^2$ at $\\omega = \\omega_0$ is $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If the true frequency is $\\omega_0$ then the maximum peak in the periodogram should have a height\n",
    "\n",
    "$$P(\\omega_0) = {N \\over 4} (a_0^2 + b_0^2)$$\n",
    "\n",
    "and standard deviation\n",
    "$$      \\sigma_P(\\omega_0)  = {{\\sqrt{2}} \\over 2} \\, {\\sigma^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Properties of LS and the periodogram\n",
    "\n",
    "Some properties of Lomb-Scargle and the periodogram:\n",
    "- The expected heights of the peaks in a periodogram don't depend on $\\sigma$ but their variation in height do.\n",
    "- For $P_{\\rm LS}(\\omega_0)$, with no noise the peak approaches 1. As noise increases, $P_{\\rm LS}(\\omega_0)$ decreases and is ``buried'' in the background  noise.\n",
    "- To estimate the uncertainty on the peaks we can use a bootstrap approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalized Lomb-Scargle\n",
    "\n",
    "The Lomb-Scargle periodogram is a fit to a model:\n",
    "\n",
    "$$y(t)=a \\sin(\\omega t)+b \\cos(\\omega t),$$\n",
    "\n",
    "whose mean is zero. In reality, the observed variability is typically around some (mean) value, not zero. We deal with this by subtracting the mean of the sample $\\overline{y}$ from the data before performing the periodogram analysis.\n",
    "\n",
    "That only works if $\\overline{y}$ is a good estimator of the mean of the distribution, $y(t)$ -- if the data is equally sampled at all phases. However, in practice, the data may not equally sample all of the phases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalized Lomb-Scargle\n",
    "\n",
    "\n",
    "For example, consider the case of a star that has a period of one day and the fact that a single optical telescope only takes data at night. In that case you we might get something like the top panel below:\n",
    "\n",
    "![Ivezic, Figure 10.16](http://www.astroml.org/_images/fig_LS_sg_comparison_3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The \"generalized\" Lomb-Scargle approach (also implemented in astroML) can help with this as can be seen in the bottom panel above. It fits the model with non-zero mean:\n",
    "\n",
    "$$y(t)=a \\sin(\\omega t)+b \\cos(\\omega t) + C$$\n",
    "\n",
    "See also [Figure 10.16](http://www.astroml.org/book_figures/chapter10/fig_LS_sg_comparison.html) in the textbook (and note the erratum!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multiband LS periodograms\n",
    "\n",
    "One of the issues that we are going to be dealing with in astronomy over the next decade as the LSST project comes online is the problem of sparsely-sampled light curves, but with multiple light curves for each object -- one for each \"bandpass\" as seen below:\n",
    "\n",
    "<img src=\"figures/multibandLS.png\" style=\"float: right; width: 100%; margin-right: 1%;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The generalized LS was an extension to account for the mean value. We can build on this to account for multiple bands by fitting for a global period, global offset, and a per-band offsets.\n",
    "\n",
    "$\\begin{eqnarray}\n",
    "  &y_k(t|\\omega,\\theta) =\n",
    "  \\theta_0 + \\sum_{n=1}^{N_{base}} \\left[\\theta_{2n - 1}\\sin(n\\omega t) + \\theta_{2n}\\cos(n\\omega t)\\right] +&\\nonumber\n",
    "  &\\theta^{(k)}_0 + \\sum_{n=1}^{N_{band}} \\left[\\theta^{(k)}_{2n - 1}\\sin(n\\omega t) + \\theta^{(k)}_{2n}\\cos(n\\omega t)\\right].&\n",
    "\\end{eqnarray}$\n",
    "\n",
    "The total number of parameters for $K$ filters is then $M_K = (2N_{base} + 1) + K(2N_{band} + 1)$. \n",
    "\n",
    "To keep the parameters constrained we apply regularization (we'll learn about it when we discuss regression).\n",
    "\n",
    "The important feature of this model is that _all bands_ share the same base parameters, $\\theta$, while their offsets $\\theta^{(k)}$ are determined individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-band Periodogram\n",
    "\n",
    "VanderPlas and Ivezic (2015, ApJ 812, 18) \n",
    "<img src=\"figures/VdPIvezic1.png\" style=\"float: left; width: 90%; margin-right: 1%;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figures/VdPIvezic2.png\" style=\"float: left; width: 75%; margin-right: 1%;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preview to Classification\n",
    "\n",
    "With parameters of a periodic model in hand, we can attempt to classify our sources.  Either using supervised methods if we have labeled examples or unsupervised methods if we do not.\n",
    "\n",
    "The examples below show that a sample of variable stars can be divided into different groups.  The first plot shows an unsupervised clustering analysis and the second is a supervised GMMB classification.\n",
    "\n",
    "![Ivezic, Figure 10.20](http://www.astroml.org/_images/fig_LINEAR_clustering_1.png)\n",
    "\n",
    "![Ivezic, Figure 10.22](http://www.astroml.org/_images/fig_LINEAR_GMMBayes_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Statistical Inference\n",
    "\n",
    "In Bayesian inference, we evaluate the **posterior probability** by using\n",
    "** data likelihood** and **prior** information: \n",
    " \n",
    "$$p(M,\\theta \\,|\\,D,I) = \\frac{p(D\\,|\\,M,\\theta,I)\\,p(M,\\theta\\,|\\,I)}{p(D\\,|\\,I)},$$\n",
    "\n",
    "The prior can be expanded as \n",
    "$$p(M,\\theta\\,|\\,I) = p(\\theta\\,|\\,M,I)\\,p(M\\,|\\,I).$$\n",
    " \n",
    "It is often the case that $p(D\\,|\\,I)$ is not evaluated explictly since the posterior probability \n",
    "can be (re)normalized. \n",
    "\n",
    "**The Bayesian Statistical Inference process** is then\n",
    "* formulate the likelihood, $p(D\\,|\\,M,\\theta,I)$\n",
    "* chose a prior $p(M,\\theta\\,|\\,I)$, which incorporates *other information beyond the data in $D$*\n",
    "* determine the posterior pdf, $p(M,\\theta \\,|\\,D,I)$\n",
    "* search for the model parameters that maximize $p(M,\\theta \\,|\\,D,I)$ \n",
    "* quantify the uncertainty of the model parameter estimates (credible region)\n",
    " \n",
    " \n",
    "### Bayesian Model Comparison\n",
    "\n",
    "To determine which model is better we compute the ratio of the posterior probabilities or the **odds ratio** for two models as\n",
    "$$O_{21} \\equiv \\frac{p(M_2|D,I)}{p(M_1|D,I)}.$$\n",
    "\n",
    "The posterior probability that the model $M$ is correct given data $D$ is\n",
    "$$p(M|D,I) = \\frac{p(D|M,I)p(M|I)}{p(D|I)},$$\n",
    "and the odds ratio can ignore $p(D|I)$ since it will be the same for both models. \n",
    "\n",
    "We get \n",
    "$$O_{21} = \\frac{p(D\\,|\\,M_2,I)\\,p(M_2\\,|\\,I)}{p(D\\,|\\,M_1,I)\\,p(M_1\\,|\\,I)} \\equiv B_{21} \\, \\frac{p(M_2\\,|\\,I)}{p(M_1\\,|\\,I)},$$\n",
    "where $B_{21}$ is called the Bayes factor. \n",
    "\n",
    "The Bayes factor compares how well the models fit the data: it is a ratio of data likelihoods averaged over \n",
    "all allowed values of the model parameters and computed as\n",
    "$$B_{21} = \\frac{\\int p(D\\,|\\,M_2, \\theta, I) \\, p(\\theta\\,|\\,M_2, I) \\, d\\theta}{\\int p(D\\,|\\,M_1, \\theta, I) \\, p(\\theta\\,|\\,M_1, I) \\, d\\theta}. $$\n",
    " \n",
    "In other words, the Bayes factor is the ratio of **the global likelihoods for models $M_1$ and $M_2$**, \n",
    "where the global likelihood, or evidence, is a weighted average of the likelihood function, with the \n",
    "prior for model parameters acting as the weighting function.\n",
    "\n",
    "N.B. To get the best-fit model parameters, we take a derivative of the product of the likelihood function\n",
    "and priors (and equate it to 0), while to get the global likelihood we integrate it. \n",
    " \n",
    "**How do we interpret the values of the odds ratio, $O_{21}$, in practice?**\n",
    "\n",
    "Jeffreys proposed a five-step scale for \n",
    "interpreting the odds ratio, where $O_{21} > 10$ represents “strong” evidence in favor of $M_2$ ($M_2$ \n",
    "is ten times more probable than $M_1$; analogously $O_{21} < 0.1$ is “strong” evidence in favor of $M_1$), \n",
    "and $O_{21} > 100$ is “decisive” evidence ($M_2$ is one hundred times more probable than $M_1$). \n",
    "When $O_{21} < 3$, the evidence is “not worth more than a bare mention.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Approximate Bayesian Model Comparison\n",
    "\n",
    "The data likelihood, required to compute the models odds ratio, and the commonly \n",
    "used $\\chi^2$ goodness-of-fit parameter are related. By introducting a few \n",
    "assumptions and approximations, the computations of the odds ratio can be\n",
    "greatly simplified and expressed as the sum of $\\chi^2$ and a term that penalizes\n",
    "models for their parameters (when models achieve similar values of $\\chi^2$,\n",
    "the one with the smallest number of free parameters wins). \n",
    " \n",
    "We consider a one dimensional case with unknown parameter $\\mu$ and\n",
    "start with an approximate computation of the evidence $E(M)$,\n",
    "$$ E(M) = \\int p(\\{x_i\\}, \\,|\\,M, \\mu, I) \\, p(\\mu \\,|\\,M, I) \\, d \\mu .$$\n",
    "\n",
    "Our first assumption is that the prior is uniform \n",
    "$$  p(\\mu\\,|\\, M, I) = \\frac{1}{\\Delta} \\,\\,\\,\\,\\,\\, {\\rm for} \\,\\,\\, -\\frac{\\Delta}{2} < \\mu < \\frac{\\Delta}{2}$$\n",
    "and 0 otherwise. \n",
    "\n",
    "The second assumption is that the data likelihood can\n",
    "be approximated as a Gaussian around its maximum at $\\mu=\\mu_0$\n",
    "$$ p(\\{x_i\\} \\,|\\,M, \\mu, I) \\approx L(\\mu_0) \\, \\exp \\left( - \\frac{(\\mu - \\mu_0)^2}{2 \\sigma_\\mu^2} \\right). $$\n",
    "\n",
    "These assumptions lead to \n",
    "$$ E(M) \\approx \\frac{L(\\mu_0)}{\\Delta} \\, \\int_{-\\Delta/2}^{\\Delta/2} \\exp \\left( - \\frac{(\\mu - \\mu_0)^2}{2 \\sigma_\\mu^2}\\right) d\\mu, $$\n",
    "and with an additional assumption $\\sigma_\\mu \\ll \\Delta$ (data overcomes\n",
    "the prior), we get\n",
    "$$ E(M) \\approx \\frac{\\sigma_\\mu}{\\Delta} \\, L(\\mu_0) \\, \\sqrt{2\\pi}. $$\n",
    "\n",
    "Note that $E(M) \\ll L(\\mu_0)$, because $\\sigma_\\mu \\ll \\Delta$. In multi-dimensional\n",
    "case, each model parameter constrained by the model carries a similar \n",
    "multiplicative penalty, proportional to $\\sigma_\\mu/\\Delta$, when computing the \n",
    "Bayes factor. If a parameter, or a degenerate parameter combination, is unconstrained \n",
    "by the data (i.e., $\\sigma_\\mu \\approx \\Delta$), there is no penalty.  \n",
    "The odds ratio can justify an additional model parameter **only if this penalty is offset** by either \n",
    "an increase of the maximum value of the data likelihood, $L(\\mu_0$), or by the ratio \n",
    "of prior model probabilities, $p(M_2|I)/p(M_1|I)$. If both of these quantities are \n",
    "similar for the two models, the one with fewer parameters typically wins.\n",
    "\n",
    "If a model is well constrained by the data, for each of $k$ constrained\n",
    "parameters, $\\sigma_\\mu \\propto 1/\\sqrt{N}$, where $N$ is the number of data \n",
    "points. Therefore,\n",
    "$$ E(M) \\propto L(\\mu_0) \\, \\left(\\sqrt{N}\\right)^k. $$\n",
    " \n",
    "In order to establish connection with $\\chi^2$ via $L(\\mu_0) =\n",
    "\\exp\\left(-\\frac{\\chi^2}{2}\\right)$, we define for a model the **Bayesian information \n",
    "criterion (BIC)** as $BIC \\equiv -2\\ln{E(M)}$, and finally obtain  \n",
    "$${\\rm BIC} = -2 \\ln [L_0(M)] + k \\ln N.$$ \n",
    "The 1st term on the RHS is equal to model's $\\chi^2$ (under the assumption \n",
    "of normality; note that this is *not* $\\chi^2$ per degree of freedom!) and \n",
    "the 2nd term on the RHS penalizes complex models relative to simple ones.\n",
    "\n",
    "In summary, when multiple models are considered, their BIC values, computed as\n",
    "$${\\rm BIC} = \\chi^2 + k \\ln N,$$ \n",
    "where $N$ is the number of data points and $k$ is the number of constrained\n",
    "model parameters, are compared and the model with the smallest BIC value wins.   \n",
    " \n",
    "**N.B.** BIC is an approximation and might not be valid if the underlying \n",
    "assumptions (data ovecoming prior and Gaussian likelihood) are not met! In general, it is better to compute the odds ratio when computationally feasible.\n",
    "\n",
    "   \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Coin Flip as an Example of Bayesian Model Comparison\n",
    "\n",
    "Let's look at an example we already discussed: the coin flip. Let's assume we have\n",
    "N draws and k are success (say, heads). \n",
    "\n",
    "We will compare two hypotheses; \n",
    "\n",
    "**M1**: the coin has a known heads probability $b_\\ast$ (say, a fair coin with $b_\\ast=0.5$), and \n",
    "\n",
    "**M2**: the heads probability $b$ is unknown, with a uniform prior in the range 0–1. \n",
    "Note that the prior for model M1 is a delta function, $\\delta(b-b_\\ast)$. \n",
    "\n",
    "Given a model parametrized by the probability of success $b$, the likelihood that the data set \n",
    "contains k outcomes equal to 1 is given by \n",
    "  $$    p(k\\,|\\,b, N) = \\frac{N!}{k! \\, (N-k)!} \\, b^k \\, (1-b)^{N-k} $$\n",
    "\n",
    "For model M2 the prior for $b$ is flat in the range 0-1 and the product of the \n",
    "data likelihood and prior is same as above. However, for model M1 the prior is a \n",
    "delta function $\\delta(b-b_\\ast)$ and we get for the product of the \n",
    "data likelihood and prior  \n",
    "$$    p(k\\,|\\,b_\\ast, N, M1)\\,p(b|M1, I) = \\frac{N!}{k! \\, (N-k)!} \\, b_\\ast^k \\, (1-b_\\ast)^{N-k}. $$\n",
    "\n",
    "Consequently, the odds ratio is given by \n",
    "$$ O_{21} = \\int_0^1 \\left(\\frac{b}{b_\\ast}\\right)^k \\left(\\frac{1-b}{1-b_\\ast}\\right)^{N-k} db, $$\n",
    "as illustrated in the following figure. \n",
    "\n",
    " \n",
    "![OddsRatio](figures/odds.jpg)\n",
    "\n",
    "This figure (from the textbook) illustrates the behavior of $O_{21}$ as a function of $k$\n",
    "for two different values of N and for two different values of $b_\\ast$: $b_\\ast = 0.5$ \n",
    "(M1: the coin is fair) and $b_\\ast = 0.1$. As the figure shows, the ability to distinguish \n",
    "the two hypothesis **improves** with the sample size. For example, when $b_\\ast= 0.5$ and \n",
    "k/N = 0.1, the odds ratio in favor of M2 increases from about 9 for N = 10 to about \n",
    "263 for N = 20. When k = $b_\\ast N$, the odds ratio is 0.37 for N = 10 and 0.27 for N = 20. \n",
    "In other words, **the simpler model is favored by the data**, and the support strengthens \n",
    "with the sample size. \n",
    "\n",
    "It is easy to show by integration of the above equation for $O_{21}$ that \n",
    "$O_{21}= \\sqrt{\\pi/(2N)}$ when k = $b_\\ast N$ and $b_\\ast = 0.5$. For example, to build strong \n",
    "evidence that a coin is fair, $O_{21} < 0.1$, it takes as many as N $>$ 157 tosses. With \n",
    "N = 10,000, the heads probability of a fair coin is measured with a precision of 1% and\n",
    "the corresponding odds ratio is $O_{21} \\approx 1/80$, approaching Jeffreys’ decisive \n",
    "evidence level. \n",
    " \n",
    "In **frequentist approach**, we would ask whether we can reject the null hypothesis that \n",
    "our coin is fair. We would ask whether a given $k$ is a very unusual outcome (at some \n",
    "significance level $\\alpha$, say $\\alpha=0.05$, which corresponds to about \"2$\\sigma$\"\n",
    "deviation) for a fair coin with $b_\\ast = 0.5$ and with\n",
    "a given N. In the **Bayesian approach**, we offer an alternative hypothesis that the coin \n",
    "has an unknown heads probability. While this probability can be estimated from provided \n",
    "data ($b_0$), **we consider all the possible values** of $b_0$ when comparing the two proposed \n",
    "hypotheses. \n",
    "\n",
    "As a numerical example, let's consider N=20 and k=16. Using the results discussed earlier, \n",
    "we find that the scatter around the expected value $k_0 = b_\\ast N$ = 10 is $\\sigma_k = 2.24$. \n",
    "Therefore, k = 16 is about 2.7$\\sigma_k$ away from $k_0$, and at the adopted significance \n",
    "level $\\alpha=0.05$ we **reject the null hypothesis** (this rejection means that it is unlikely that k = 16 would have arisen by chance). Of course, k = 16 does **not** imply \n",
    "that it is impossible that the coin is fair (infrequent events happen, too!).\n",
    "\n",
    "As shown in the above figure, the chosen parameters (N=20 and k=16) correspond to the \n",
    "Bayesian **odds ratio** of about 10 in favor of hypothesis M2.  \n",
    "\n",
    "\n",
    "### Approximate Bayesian Model Comparison\n",
    "\n",
    "Let's now use BIC to address the same problem and illustrate how model M2 gets \n",
    "penalized for its free parameter. Essentially, we want an approximation for the\n",
    "integral expression for $O_{21}$\n",
    "$$ O_{21} = \\int_0^1 \\left(\\frac{b}{b_\\ast}\\right)^k \\left(\\frac{1-b}{1-b_\\ast}\\right)^{N-k} db, $$\n",
    " \n",
    "We can approximate \n",
    "$$ E(M2) \\approx \\sqrt{2\\pi} \\, L(b_0) \\, \\sigma_b, $$\n",
    "where $b_0 = k/N$ and for largish N we have $\\sigma_b \\approx \\sqrt{b_0(1-b_0)/N}$. \n",
    "\n",
    "For M1, we have exact result because the prior is $\\delta$ function\n",
    "$$ E(M1) = \\frac{N!}{k! \\,(N=k)!} b_\\ast^k (1-b_\\ast)^{N-k}.$$\n",
    "\n",
    "And the odds ratio becomes\n",
    "$$ O_{21} \\approx \\sqrt{2\\pi} \\, \\sigma_b \\,\n",
    "\\left(\\frac{b_0}{b_\\ast}\\right)^k \\left(\\frac{1-b_0}{1-b_\\ast}\\right)^{N-k} =\n",
    "\\sqrt{2\\pi}\\, \\sqrt{\\frac{b_0(1-b_0)}{N}} \\, \\left(\\frac{b_0}{b_\\ast}\\right)^k \\left(\\frac{1-b_0}{1-b_\\ast}\\right)^{N-k} .$$\n",
    "\n",
    "Now we can explicitly see that the evidence in favor of model M2 decreases (the model is “penalized”) proportionally to the posterior pdf width of its free parameter, $\\sigma_b$. \n",
    "\n",
    "If indeed $b_0 \\approx b_\\ast$, model M1 wins because it explained the data without\n",
    "any free parameter. On the other hand, the evidence in favor of M2 increases as the \n",
    "data-based value $b_0$ becomes very different from the prior $b_\\ast$ claimed by model M1.\n",
    "** Model M1 becomes disfavored because it is unable to explain the observed data.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Markov Chain Monte Carlo\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Consider the problem of estimating location and scale parameters\n",
    "for a sample drawn from Gaussian distribution that we introduced earlier.\n",
    "We had a two-dimensional posterior pdf for $\\mu$ and $\\sigma$:\n",
    "\n",
    "![BayesSlide1](figures/Lgauss.jpg)\n",
    "\n",
    "We obtained posterior pdf for $\\mu$ and $\\sigma$ by intergrating\n",
    "the two-dimensional posterior pdf for $\\mu$ and $\\sigma$ over \n",
    "$\\sigma$ and $\\mu$, respectively: \n",
    "\n",
    "![BayesSlide1](figures/LgaussM.jpg)\n",
    "\n",
    "\n",
    "It was easy to numerically integrate the posterior pdf, as well \n",
    "as to find its maximum, using brute force grid search because\n",
    "it was only a two-dimesional problem. With 100 grid points per\n",
    "coordinate it was only $10^4$ values. However, even in a case\n",
    "of rather simple 5-dimensional problem (as we'll discuss later \n",
    "today), we'd have $10^{10}$ values! And often we work with models \n",
    "of much higer dimensionality (it can be thousands). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** We need a better method than the brute force grid search! **\n",
    "\n",
    "For example, if we could generate a sample of $\\{\\mu_i,\\sigma_i\\}$ \n",
    "drawn from the posterior pdf for $\\mu$ and $\\sigma$, we could simply\n",
    "get posterior pdf for $\\mu$ and $\\sigma$ by plotting histograms of \n",
    "$\\mu$ and $\\sigma$ (similar to the above figure). As simple as that! \n",
    "\n",
    "But how can we get such samples? ** By using computers!** And MCMC :) \n",
    "\n",
    "First we'll say a few words about Monte Carlo in general, and \n",
    "then we'll talk about a special kind of Monte Carlo called \n",
    "Markov Chain Monte Carlo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of the general problem\n",
    "\n",
    "What we want to be able to do is to evaluate multi-dimensional \n",
    "($\\theta$ is a k-dimensional vector) integrals of the form \n",
    "$$ I = \\int g(\\theta) \\, p(\\theta) \\, d\\theta,$$\n",
    "where for simplicity posterior pdf is described as\n",
    "$$ p(\\theta) \\equiv p(M,\\theta \\,|\\,D,I) \\propto p(D\\,|\\,M,\\theta,I)\\,p(M,\\theta\\,|\\,I). $$\n",
    "\n",
    "For example:\n",
    "\n",
    "1) **Marginalization**: if the first $P$ elements of $\\theta$ are the sought\n",
    "after model parameters, and the next $k-P$ parameters are nuisance \n",
    "parameters, when marginalizing $p(\\theta)$ over nuisance parameters\n",
    "we have $g(\\theta) = 1$ and we integrate over space spanned by $k-P$ \n",
    "nuisance parameters. \n",
    "\n",
    "2) **Point estimates** for the posterior: if we want the mean of a model\n",
    "parameter $\\theta_m$, then $g(\\theta) = \\theta_m$ and we integrate over\n",
    "all model parameters. \n",
    "\n",
    "3) **Model comparison**: here $g(\\theta) = 1$ and we integrate over all model\n",
    "parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Monte Carlo Methods \n",
    "\n",
    "What you need is a computer that can generate (pseudo)random numbers and then you\n",
    "solve a lot of hard problems. Let' start with an easy problem of one-dimensional\n",
    "numerical integration.\n",
    "\n",
    "Assume that you can generate a distribution of M random numbers $\\theta_j$ uniformly sampled \n",
    "within the integration volume V. Then our interval can be evaluated as \n",
    "$$ I = \\int g(\\theta) \\, p(\\theta) \\, d\\theta = \\frac{V}{M} \\sum_{j=1}^M g(\\theta_j) \\, p(\\theta_j).$$\n",
    "    \n",
    "Note that in 1-D we can write a similar expression \n",
    "$$ I = \\int f(\\theta) \\, d\\theta = \\Delta \\, \\sum_{j=1}^M g(\\theta_j) \\, p(\\theta_j).$$\n",
    "\n",
    "where $ f(\\theta) = g(\\theta) \\, p(\\theta) $, and it is assumed that the values\n",
    "$\\theta_j$ are sampled on a regular grid with the step $\\Delta = V/M$ ($V$ here is the\n",
    "length of the sampling domain). This expression is the simplest example of\n",
    "numerical integration (\"rectangle rule\", which amounts to approximating $f(\\theta)$\n",
    "by a piecewise constant function).\n",
    "\n",
    "The reason why we expressed $f(\\theta)$\n",
    "as a product of $g(\\theta)$ and $p(\\theta)$ is that, as we will see shortly,\n",
    "we can generate a sample drawn from $p(\\theta)$ (instead of sampling on a \n",
    "regular grid), and this greatly improves the performance of numerical integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Markov Chains\n",
    "\n",
    "A number of methods exist that are much more efficient than generic Monte Carlo integration. \n",
    "The most popular group of techniques is known as Markov chain Monte Carlo (MCMC) methods. \n",
    "\n",
    "MCMC returns a sample of points, or **chain**, from the k-dimensional parameter space, with \n",
    "a distribution that is **asymptotically proportional** to $p(\\theta)$. The constant of \n",
    "proportionality is not important in the first class of problems listed above. In model \n",
    "comparison problems, the proportionality constant must be known and we will return to this \n",
    "point later.\n",
    "\n",
    "Given such a chain of length M, the integral can be estimated as\n",
    "$$ I = \\int g(\\theta) \\, p(\\theta) \\, d\\theta = \\frac{1}{M} \\sum_{j=1}^M g(\\theta_j).$$\n",
    "\n",
    "Again, here the values of $\\theta$ are **not** sampled **uniformly** from the volume;\n",
    "they are sampled **proportionally** to $p(\\theta)$! Note that there is no $p(\\theta_j)$ \n",
    "term next to $g(\\theta_j)$ because the proper weighting in the sum is taken care of \n",
    "by the sample itself! \n",
    "\n",
    "** How do we get such a chain? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is a Markov Chain?\n",
    "\n",
    "A Markov Chain is **defined as a sequence of random variables where a parameter depends \n",
    "*only* on the preceding value.**  Such processes are \"memoryless\".  \n",
    " \n",
    "Mathematically, we have\n",
    "$$p(\\theta_{i+1}|\\{\\theta_i\\}) = p(\\theta_{i+1}|\\,\\theta_i).$$\n",
    "\n",
    "\n",
    "In our context, $\\theta$ can be thought of as a vector in multidimensional space, and a\n",
    "realization of the chain represents a path through this space.\n",
    "\n",
    "To reach an equilibrium, or stationary, distribution of positions, it is necessary that the transition probability is symmetric:\n",
    "$$    p(\\theta_{i+1}|\\,\\theta_i) = p(\\theta_i |\\, \\theta_{i+1}). $$\n",
    "This condition is called the **detailed balance** or **reversibility condition**. It\n",
    "shows that the probability of a jump between two points does not depend on the direction of the jump.\n",
    "\n",
    "There are various algorithms for producing Markov chains that reach some prescribed\n",
    "equilibrium distribution, $p(\\theta)$. The use of resulting chains to perform Monte Carlo\n",
    "integration is called *Markov chain Monte Carlo* (MCMC).\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is a Markov Chain?\n",
    "\n",
    "Let's see an example from\n",
    "[Andrieu et al. ``An Introduction to MCMC for Machine Learning\" (includes a few pages of history)\"](http://www.cs.princeton.edu/courses/archive/spr06/cos598C/papers/AndrieuFreitasDoucetJordan2003.pdf)\n",
    "\n",
    "<img src=\"figures/Andrieu1.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img src=\"figures/Andrieu2.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img src=\"figures/Andrieu3.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Markov Chain Monte Carlo\n",
    "\n",
    "The modern version of the Markov Chain Monte Carlo method was invented in the late 1940s by Stanislaw Ulam, while he was working on nuclear weapons projects at the Los Alamos National Laboratory. The name Monte Carlo\n",
    "was given to the method by Nick Metropolis, who then invented the Metropolis sampler, which evolved into\n",
    "one of the most famous MCMC algorithms, the Metropolis-Hastings algorithm. \n",
    "\n",
    "Algorithms for generating Markov chains are numerous and greatly vary in complexity\n",
    "and applicability. Many of the most important ideas were generated in physics, especially\n",
    "in the context of statistical mechanics, thermodynamics, and quantum field theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transition Kernels\n",
    "\n",
    "In order for a Markov chain to reach a stationary distribution proportional to $p(\\theta)$,\n",
    "the probability of arriving at a point $\\theta_{i+1}$ must be proportional to $p(\\theta_{i+1})$,\n",
    "$$ p(\\theta_{i+1}) = \\int  T(\\theta_{i+1}|\\theta_i)  \\,   p(\\theta_i) \\,    d \\theta_i, $$\n",
    "where the transition probability $T(\\theta_{i+1}|\\theta_i)$ is called the **jump kernel** or\n",
    "**transition kernel** (and it is assumed that we know how to compute $p(\\theta_i)$).\n",
    "\n",
    "This requirement will be satisfied when the transition probability satisfies **the detailed\n",
    "balance condition**\n",
    "\n",
    "$$ T(\\theta_{i+1}|\\theta_i)  \\,  p(\\theta_i) = T(\\theta_i|\\theta_{i+1})  \\,  p(\\theta_{i+1}). $$\n",
    "\n",
    "*Various MCMC algorithms differ in their choice of transition kernel*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Markov Chain Monte Carlo\n",
    "\n",
    "*Various MCMC algorithms differ in their choice of transition kernel*\n",
    "\n",
    "**The Metropolis-Hastings algorithm** adopts the kernel\n",
    "\n",
    "$$  T(\\theta_{i+1}\\,|\\,\\theta_i) =  p_{\\rm acc}(\\theta_i,\\theta_{i+1}) \\, K(\\theta_{i+1}\\,|\\,\\theta_i), $$\n",
    "\n",
    "where the proposed density distribution $K(\\theta_{i+1}\\,|\\,\\theta_i,)$ is an *arbitrary* function. A Gaussian distribution centered on $\\theta_i$ is often used for $K(\\theta_{i+1}|\\theta_i)$.\n",
    "\n",
    "The proposed point $\\theta_{i+1}$ is randomly accepted with the acceptance probability\n",
    "\n",
    "$$   p_{\\rm acc}(\\theta_i,\\theta_{i+1}) = { K(\\theta_i\\,|\\,\\theta_{i+1}) \\,  p(\\theta_{i+1}) \\over\n",
    "                       K(\\theta_{i+1}\\,|\\,\\theta_i) \\,  p(\\theta_i) } $$\n",
    "\n",
    "(when exceeding 1, the proposed point $\\theta_{i+1}$ is always accepted). ** When $\\theta_{i+1}$ is rejected, $\\theta_i$ is added to the chain instead. **\n",
    "\n",
    "The original Metropolis algorithm is based on a symmetric proposal distribution,\n",
    "$K(\\theta_{i+1}|\\theta_i) =  K(\\theta_i|\\theta_{i+1})$, which then cancels out from\n",
    "the acceptance probability. **In this case, $\\theta_{i+1}$ is always accepted if\n",
    "$p(\\theta_{i+1}) > p(\\theta_i)$, and if not, then it is accepted with a probability\n",
    "$p(\\theta_{i+1})/p(\\theta_i)$.**\n",
    "\n",
    "\n",
    "This algorithm guarantees that the chain will reach an equilibrium, or stationary, distribution, and it will approximate a sample drawn from $p(\\theta)$! \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Markov Chain Monte Carlo\n",
    "\n",
    "**In summary, the Metropolis-Hastings algorithm consists of these steps:**\n",
    "\n",
    "1) given $\\theta_i$ and $K(\\theta_{i+1}|\\theta_i)$, draw a proposed value for $\\theta_{i+1}.$ \n",
    "\n",
    "2) compute acceptance probability $p_{\\rm acc}(\\theta_i,\\theta_{i+1})$.\n",
    "\n",
    "3) draw a random number between 0 and 1 from a uniform distribution; if it smaller than\n",
    "   $p_{\\rm acc}(\\theta_i,\\theta_{i+1})$, then accept $\\theta_{i+1}$.\n",
    "   \n",
    "4) if $\\theta_{i+1}$ is accepted added it to the chain, if not, add $\\theta_{i}$ to the chain.\n",
    "\n",
    "5) use the chain (of $\\theta$ values) for inference; e.g. a histogram of $\\theta$ is\n",
    "  an estimator of the posterior pdf for $\\theta$, $p(\\theta)$, and the expectation value for \n",
    "  $\\theta$ can be computed from \n",
    "  $$ I = \\int g(\\theta) \\, p(\\theta) \\, d\\theta = \\frac{1}{M} \\sum_{j=1}^M \\theta_j.$$\n",
    "\n",
    "where M is the number of elements in the chain (e.g. \n",
    "the expectation value for $\\theta$ is simply the mean value of chain elements). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Toy Example: A Two-state System\n",
    "\n",
    "Let's consider for a moment **a two-state example,with states A and B**. It would be a stretch to call it an MCMC example, but it does address one of the key aspects of MCMC: how to set the rejection (or acceptance)\n",
    "probability for a new \"proposal\".\n",
    "\n",
    "Let's assume that the target probabilities are $t(A)$ and $t(B)$, with $t(A) > t(B)$  without a loss of generality (and of course $t(A) + t(B)=1$). Say, $t(A)=0.9$ and $t(B)=0.1$. **What should the transition probability be for this system to be stationary?**\n",
    "\n",
    "<img src=\"figures/digits.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "The last condition, accepting B with a probability $T(B|A)=p(B)/p(A)$, **guarantees**\n",
    "that we will eventually arrive to a stationary distribution, and that distribution \n",
    "will converge to the input desired distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementing this in practice\n",
    "\n",
    "In practice, Metropolis-Hastings algorithm implements this detailed balance by **proposing a jump to a new state** and then deciding **whether to accept or reject it**. How do we set the rejection (or acceptance) probability for a new \"proposal\"?\n",
    "\n",
    "* Let's assume that the target probabilities are $p(A)$ and $p(B)$, with $p(A) > p(B)$.\n",
    "Say, $p(A)=0.9$ and $p(B)=0.1$.\n",
    "\n",
    "* We want a sample of $N$ values that can be either A or B, with $p(A)N$ in state A and $p(B)N$ in state B.\n",
    "\n",
    "* We begin by drawing a sample of $N$, say $N=1000$, random values that can be either \n",
    "A or B. We expect that we will have about N/2 values of A and about N/2 values of B. \n",
    "This is **not** what we want! (we want to have 900 A values and 100 B values, not\n",
    "500 and 500). Therefore, we need to ** take a fraction of proposed B values and\n",
    "re-assign them to A** (in other words, *reject* them from the proposed B subset). \n",
    "How do we choose them? \n",
    "\n",
    "* In a steady state described by the target probabilities, we expect that out of N/2 \n",
    "proposed B values, about $p(A)N/2$ were drawn when the system was in A state, and \n",
    "about $p(B)N/2$ when the system was in B state. Hence, in the former case the system \n",
    "is in the more probable A state and the proposal is to move to the less probable B state.\n",
    "Again, if we accept all proposals, we will not end up with the desired distribution. \n",
    "Instead, we need to **reject a fraction $x_A$ of these $p(A)N/2$ proposals to adopt \n",
    "(the less probable) B state** and thus **accept (stick with) $x_A p(A) N/2$ current A states**. \n",
    "\n",
    "* When we do that, we will have a total of $\\left(N/2 + x_A*p(A)N/2\\right)$ A states, and we want it \n",
    "to be equal to $p(A)N$ (so the distribution doesn't change):\n",
    "$$ \\frac{N}{2} + x_A\\,p(A)\\,\\frac{N}{2} = t(A)\\,N.$$\n",
    "* Therefore, the ***rejection*** fraction must be:\n",
    "$$ x_A = 1 - p(B)/p(A) $$\n",
    "so the ***acceptance*** probability in cases when the proposal is to go from state A to state B\n",
    "must be \n",
    "$$p_{acc}(B|A) =  p(B)/p(A).$$ \n",
    "In all other cases (A proposed to go to A, and B proposed to go to either state), the proposal is always accepted. \n",
    "\n",
    "In summary, **the acceptance probability** for the two-state system is \n",
    "\n",
    "The acceptance probability for the two-state system is\n",
    "\n",
    "1. in A, proposed to go to A: accept\n",
    "1. in A, proposed to go to B: accept in p(B)/p(A) cases\n",
    "1. in B, proposed to go to A: accept\n",
    "1. in B, proposed to go to B: accept\n",
    "\n",
    "Let's now do the 10-state example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Markov Chain Monte Carlo\n",
    "\n",
    "*Various MCMC algorithms differ in their choice of transition kernel*\n",
    "\n",
    "**The Metropolis-Hastings algorithm** adopts acceptance probability\n",
    "$$ p_{\\rm acc}(\\theta_i,\\theta_{i+1}) = { p(\\theta_{i+1}) \\over p(\\theta_i) }, $$\n",
    "where the proposed point $\\theta_{i+1}$ is drawn from an *arbitrary* symmetric density distribution $K(\\theta_{i+1}\\,|\\,\\theta_i)$. A Gaussian distribution centered on \n",
    "$\\theta_i$ is often used for $K(\\theta_{i+1}|\\theta_i)$.\n",
    "\n",
    "\n",
    "\n",
    "## Caveats\n",
    "\n",
    "Although $K(\\theta_{i+1}|\\theta_i)$ satisfies a Markov chain requirement that it\n",
    "must be a function of only the current position $\\theta_i$, it takes a number\n",
    "of steps to reach a stationary distribution from an initial arbitrary position $\\theta_0$.\n",
    "**These early steps are called the \"burn-in\" and need to be discarded in analysis.**\n",
    "There is no general theory for finding transition from the burn-in phase to\n",
    "the stationary phase; several methods are used in practice. Gelman and Rubin\n",
    "proposed to generate a number of chains and then compare the ratio of\n",
    "the variance between the chains to the mean variance within the chains (this\n",
    "ratio is known as the $R$ statistic). For stationary chains, this ratio will\n",
    "be close to 1.  \n",
    "\n",
    "When the posterior pdf is multimodal, the simple Metropolis--Hastings algorithm can\n",
    "become stuck in a local mode and not find the globally best mode within a reasonable\n",
    "running time. There are a number of better algorithms, such as Gibbs sampling, parallel\n",
    "tempering, various genetic algorithms, and nested sampling.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"figures/Andrieu4.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian priors\n",
    "\n",
    "Priors can be **informative** or **uninformative**.  As it sounds, informative priors are based on existing information (including previously obtained data, but not the data considered right now) that might be available.  Uniformative priors can be thought of as \"default\" priors, i.e., what your prior is when you never used\n",
    "any data, e.g, a \"flat\" prior like $p(\\theta|M,I) \\propto {\\rm C}$.\n",
    "\n",
    "Detailed discussion can be found in Section 5.2 in the textbook. There are three\n",
    "main principles used to choose a prior: \n",
    "\n",
    "\n",
    "#### The Principle of Indifference\n",
    "\n",
    "Essentially this means adopting a uniform prior, though you have to be a bit careful.  Saying that an asteroid is equally likely to hit anywhere on the Earth is not the same as saying that all latitudes of impact are equally likely.  \n",
    "\n",
    "Assuming $1/6$ for a six-side die, or 1/2 for heads and tails of a fair coin, would be an example of indifference.\n",
    "\n",
    "#### The Principle of Invariance (or Consistency)\n",
    "\n",
    "This applies to location and scale invariance.  \n",
    "\n",
    "**Location invariance** suggests a uniform prior, within the accepted bounds: $p(\\theta|I) \\propto 1/(\\theta_{max}-\\theta_{min})$ for $\\theta_{min} \\le \\theta \\le \\theta_{max}$. \n",
    "\n",
    "**Scale invariance** gives us priors that look like $p(\\theta|I) \\propto 1/\\theta$, which implies a uniform\n",
    "prior for ln($\\theta$). \n",
    "\n",
    "#### The Principle of Maximum Entropy\n",
    "\n",
    "We will not discuss it here - for more details, see Section 5.2.2 in the textbook.\n",
    " \n",
    "It is often true that Bayesian analysis and traditional MLE are essentially equivalent.  \n",
    "However, in some cases, considering the priors can have significant consequences, as\n",
    "we will see later. \n",
    "\n",
    "We will skip examples of very steep priors and their consequences called in astronomy\n",
    "literature **Eddington-Malmquist** and **Lutz-Kelker** biases (see Chapter 5 in the textbook\n",
    "if you are interested). \n",
    "\n",
    "Now, let's go back to the bus arrival problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Estimating multiple parameters: heteroscedastic Gaussian as an example\n",
    "\n",
    "\n",
    "Consider the case of measuring a rod as above.  We want to know the posterior pdf for the length of the rod, $p(M,\\theta|D,I) \\equiv p(\\mu|\\{x_i\\},\\{\\sigma_i\\},I)$.\n",
    "\n",
    "For the likelihood we have\n",
    "$$L = p(\\{x_i\\}|\\mu,I) = \\prod_{i=1}^N \\frac{1}{\\sigma_i\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma_i^2}\\right).$$\n",
    "\n",
    "**In the Bayesian case, we also need a prior.**  We'll adopt a uniform distribution given by\n",
    "$$p(\\mu|I) = C, \\; {\\rm for} \\; \\mu_{\\rm min} < \\mu < \\mu_{\\rm max},$$\n",
    "where $C = \\frac{1}{\\mu_{\\rm max} - \\mu_{\\rm min}}$ between the min and max and is $0$ otherwise.\n",
    "\n",
    "The log of the posterior pdf is then\n",
    "$$\\ln L = {\\rm constant} - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma_i^2}.$$\n",
    "\n",
    "This is exactly the same as we saw before, except that the value of the constant is different.  Since the constant doesn't come into play, we get the same result as before:\n",
    " \n",
    "$$\\mu^0 = \\frac{\\sum_i^N (x_i/\\sigma_i^2)}{\\sum_i^N (1/\\sigma_i^2)},$$\n",
    "with uncertainty\n",
    "$$\\sigma_{\\mu} = \\left( \\sum_{i=1}^N \\frac{1}{\\sigma_i^2}\\right)^{-1/2}.$$\n",
    " \n",
    " We get the same result because we used a flat prior. If the case were homoscedastic instead of heteroscedastic, we obviously would get the result from our first example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Estimating multiple parameters: heteroscedastic Gaussian as an example\n",
    "\n",
    "\n",
    "Now let's consider the case where **$\\sigma$ is not known**, but rather it needs to be determined from the data, too.\n",
    "\n",
    "In this case, the posterior pdf that we seek is not $p(\\mu|\\{x_i\\},\\{\\sigma_i\\},I)$, but rather $p(\\mu,\\sigma|\\{x_i\\},I)$.\n",
    "\n",
    "As before we have\n",
    "$$L = p(\\{x_i\\}|\\mu,\\sigma,I) = \\prod_{i=1}^N \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right),$$\n",
    "except that now $\\sigma$ is uknown.\n",
    "\n",
    "Our Bayesian prior is now 2D instead of 1D and we'll adopt \n",
    "$$p(\\mu,\\sigma|I) \\propto \\frac{1}{\\sigma},\\; {\\rm for} \\; \\mu_{\\rm min} < \\mu < \\mu_{\\rm max} \\; {\\rm and} \\; \\sigma_{\\rm min} < \\sigma < \\sigma_{\\rm max}.$$\n",
    "\n",
    "With proper normalization, we have\n",
    "$$p(\\{x_i\\}|\\mu,\\sigma,I)p(\\mu,\\sigma|I) = C\\frac{1}{\\sigma^{(N+1)}}\\prod_{i=1}^N \\exp\\left( \\frac{-(x_i-\\mu)^2}{2\\sigma^2}  \\right),$$\n",
    "where\n",
    "$$C = (2\\pi)^{-N/2}(\\mu_{\\rm max}-\\mu_{\\rm min})^{-1} \\left[\\ln \\left( \\frac{\\sigma_{\\rm max}}{\\sigma_{\\rm min}}\\right) \\right]^{-1}.$$\n",
    "\n",
    "The log of the posterior pdf is\n",
    "\n",
    "$$\\ln[p(\\mu,\\sigma|\\{x_i\\},I)] = {\\rm constant} - (N+1)\\ln\\sigma - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma^2}.$$\n",
    "\n",
    "This expression has $x_i$ in it, which isn't that helpful, but since we are assuming a Gaussian distribution, we can take advantage of the fact that the mean, $\\overline{x}$, and the variance, $V (=s^2)$, completely characterized the distribution.  So we can write this expression in terms of those variables instead of $x_i$.  Skipping over the math details (see textbook $\\S$5.6.1), we find\n",
    "\n",
    "$$\\ln[p(\\mu,\\sigma|\\{x_i\\},I)] = {\\rm constant} - (N+1)\\ln\\sigma - \\frac{N}{2\\sigma^2}\\left( (\\overline{x}-\\mu)^2 + V  \\right).$$\n",
    "\n",
    "Note that this expression only contains the 2 parameters that we are trying to determine: $(\\mu,\\sigma)$ and 3 values that we can determine directly from the data: $(N,\\overline{x},V)$. A side note: these three data-based values \n",
    "fully encapsulate our dataset and are called *sufficient statistics*.\n",
    "\n",
    "Load and execute the next cell to visualize the posterior pdf for the case of $(N,\\overline{x},V)=(10,1,4)$.  Remember to change `usetex=True` to `usetex=False` if you have trouble with the plotting.  Try changing the values of $(N,\\overline{x},V)$. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical Inference\n",
    "\n",
    "Statistical *inference* is about drawing conclusions from data, specifically determining the properties of a population by data sampling.\n",
    "\n",
    "Three examples of inference are:\n",
    "1. What is the best estimate for a model parameter\n",
    "2. How confident we are about our result\n",
    "3. Are the data consistent with a particular model/hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frequentist vs. Bayesian Inference\n",
    "\n",
    "There are two major statistical paradigms which address the statistical inference questions: the classical, or **frequentist** paradigm, and the **Bayesian** paradigm.\n",
    "\n",
    "While most of statistics and machine learning is based on the classical paradigm, Bayesian techniques are being embraced by the statistical and scientific communities at an ever-increasing pace.\n",
    "\n",
    "This week we begin by discussing the three main types of statistical inference from the classical (frequentist) point of view. Next week, we'll tackle the same problems from Bayesian point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some Terminology\n",
    "\n",
    "* We typically study the properties of some ***population*** by measuring ***samples*** from that population. The population doesn't have to refer to different objects. E.g., we may be (re)measuring the position of an object at rest; the population is the distribution of (an infinite number of) measurements smeared by the error, and the sample are the measurement we've actually taken.\n",
    "* A ***statistic*** is any function of the sample. For example, the sample mean is a statistic. But also, \"the value of the first measurement\" is also a statistic.\n",
    "* To conclude something about the population from the sample, we develop ***estimators***. An estimator is a statistic, a rule for calculating an estimate of a given quantity based on observed data: thus the rule (the estimator), the quantity of interest (the estimand) and its result (the estimate) can be distinguished. Sometimes *estimator* and the *estimate* are used interchangeably. Much of frequentist statistics concerns itself with development of different estimation rules -- different estimators -- and proving their properties.\n",
    "* There are ***point*** and ***interval estimators***. The point estimators yield single-valued results (example: the position of an object), while with an interval estimator, the result would be a range of plausible values (example: confidence interval for the position of an object)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "Let's talk about maximum likelihood estimation ($\\S 4.2$ in the textbook), which is relevant to both Bayesian and Frequentist approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum Likelihood Approach\n",
    "\n",
    "Maximum likelihood estimation consists of the following conceptual steps:\n",
    "\n",
    "1. **Hypothesis**: Formulate a model, a *hypothesis*, about how the data are generated. For example, this could be a statement that the data are a measurement of some quantity that come with Gaussian random errors (i.e., each measurement is equal to the true value, plus a deviation randomly drawn from the normal distribution). Models are typically described using a set of model parameters $\\boldsymbol{\\theta}$, and written as $\\boldsymbol{M}(\\boldsymbol{\\theta})$.\n",
    "2. **Maximum Likelihood Estimation**: Search for the \"best\" model parameters $\\boldsymbol{\\theta}$ which maximize the ***likelihood*** $L(\\boldsymbol{\\theta}) \\equiv p(D|M)$. This search yields the MLE *point estimates*, $\\boldsymbol{\\theta^0}$.\n",
    "3. **Quantifying Estimate Uncertainty**: Determine the confidence region for model parameters, $\\boldsymbol{\\theta^0}$. Such a confidence estimate can be obtained analytically (possibly with some approximations), but can also be done numerically for arbitrary models using general frequentist techniques, such as bootstrap, jackknife, and cross-validation.\n",
    "4. **Hypothesis Testing**: Perform hypothesis tests as needed to make other conclusions about models and point estimates. Possibly GOTO #1.\n",
    "\n",
    "While these steps represent a blueprint for the frequentist approach in general, the likelihood is just one of many possible so-called objective functions (also called fitness functions, or cost functions); other possibilities are explored briefly in §4.2.8 of the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Likelihood Function\n",
    "\n",
    "If we know the distribution from which our data were drawn (or make a hypothesis about it), then we can compute the **probability** of our data being generated.\n",
    "\n",
    "For example, for the Gaussian distribution probablity of getting a specific value of $x$ is given by:\n",
    "\n",
    "$$p(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If recall the last lecture, then you might notice that the argument of the exponential is just\n",
    "\n",
    "$$\\exp \\left(-\\frac{\\chi^2}{2}\\right).$$\n",
    "\n",
    "That is, for our gaussian distribution\n",
    "$$\\chi^2 = \\sum_{i=1}^n \\left ( \\frac{x_i-\\mu}{\\sigma}\\right)^2.$$\n",
    "\n",
    "So, maximizing the likelihood is the same as minimizing $\\chi^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Core Idea Behind Maximum Likelihood Estimators\n",
    "\n",
    "Let's say that we know that some data were drawn from a Gaussian distribution, but we don't know the $\\theta = (\\mu,\\sigma)$ values of that distribution (i.e., the parameters).\n",
    "\n",
    "Then Maximum Likelihood Estimation method tells us to think of the likelihood as a ***function of the unknown model parameters***, and ***find those that maximize the value of $L$***. Those will be our Maximum Likelihood Estimators for for the true values of the model.\n",
    "\n",
    "Simple as that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLE applied to a Homoscedastic Gaussian\n",
    "\n",
    "Let's take a look at our astrometry example, using a model where all the measurements have the same error, drawn from a normal distribution, $N(0, \\sigma)$.\n",
    "\n",
    "All errors being the same is known as having **homoscedastic** errors.  Don't be intimidated by the word, statisticians just like to sound smart, so they say \"homoscedastic\" instead of \"uniform errors\".  Later we will consider the case where the measurements can have different errors ($\\sigma_i$) which is called **heteroscedastic**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have an experiment with the set of measured positions $D=\\{x_i\\}$ in 1D with Gaussian errors, and therefore:\n",
    "\n",
    "$$L \\equiv p(\\{x_i\\}|\\mu,\\sigma) = \\prod_{i=1}^N \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right).$$\n",
    "\n",
    "Note that that is $p(\\{x_i\\})$ not $p(x_i)$, that is the probability of the full data set, not just one measurement. If $\\sigma$ is both constant and *known*, then this is a one parameter model with $k=1$ and $\\theta_1=\\mu$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For practical (and some theoretical) reasons, it's better to work with the natural logarithm of the likelihood.\n",
    "\n",
    "We define the *log-likelihood function* as ${\\rm lnL} = \\ln[L(\\theta)]$.  The maximum of this function happens at the same place as the maximum of $L$. Given all that, we have:\n",
    "\n",
    "$${\\rm lnL} = {\\rm constant} - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma^2}.$$\n",
    "\n",
    "Take a second and make sure that you understand how we got there.  It might help to remember that above, we wrote\n",
    "\n",
    "$$L = \\left( \\prod_{i=1}^n \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right) \\exp\\left( -\\frac{1}{2} \\sum \\left[\\frac{-(x_i-\\mu)}{\\sigma} \\right]^2 \\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We then determine the maximum in the same way that we always do.  It is the parameter set for which the derivative of ${\\rm lnL}$ is zero:\n",
    "\n",
    "$$\\frac{d\\;{\\rm lnL}(\\mu)}{d\\mu}\\Biggr\\rvert_{\\hat \\mu} \\equiv 0.$$\n",
    "\n",
    "That gives $$ \\sum_{i=1}^N \\frac{(x_i - \\hat \\mu)}{\\sigma^2} = 0.$$\n",
    "\n",
    "(note: we should also check that the $2^{\\rm nd}$ derivative is negative, to ensure this is the *maximum* of $L$)\n",
    "\n",
    "(also note: any constants in $\\ln L$ disappear when differentiated, so constant terms can typically be ignored.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since $\\sigma = {\\rm constant}$, that says \n",
    "$$\\sum_{i=1}^N x_i = \\sum_{i=1}^N \\hat \\mu = N \\hat \\mu.$$\n",
    "\n",
    "Thus we find that\n",
    "$$\\hat \\mu = \\frac{1}{N}\\sum_{i=1}^N x_i,$$\n",
    "which is just the arithmetic mean of all the measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Sample Mean is an ML Estimator\n",
    "\n",
    "The mean of observations drawn from a $N(\\mu, \\sigma=const)$ distribution is a maximum-likelihood estimator of the distribution's $\\mu$ parameter.\n",
    "\n",
    "We'd intuitively guess that (and we often do), but this derivation clarifies our choice: as an estimator of the real value of $\\mu$, we adopt the value $\\hat \\mu$ with which it's maximally likely for the measured data set to occur.\n",
    "\n",
    "It also exposes the ***assumptions*** behind this conclusion; namely homoscedasticity and gaussianity of errors. For example, if our errors were Cauchy-distributed, the mean of the sample won't be a good estimator (google for \"Cauchy mean\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Properties of ML Estimators\n",
    "\n",
    "Assuming the data truly are drawn from the model, ML estimators have the following useful properties:\n",
    "\n",
    "* **They are consistent estimators**; that is, they can be proven to converge to the true parameter value as the number of data points increases.\n",
    "* **They are asymptotically normal estimators**. The distribution of the parameter estimate, as the number of data points increases to infinity, approaches a normal distribution, centered at the MLE, with a certain spread. This spread can often be easily calculated and used as a confidence band around the estimate, as discussed below (see eq. 4.7).\n",
    "* **They asymptotically achieve the theoretical minimum possible variance, called the Cramér–Rao bound**. In other words, they achieve the best possible error given the data at hand; that is, no other estimator can do better in terms of efficiently using each data point to reduce the total error of the estimate (see eq. 3.33 in the textbook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantifying Estimate Uncertainty\n",
    "\n",
    "As you've seen on in the previous plot, our ML estimate of $\\mu$ is not perfect. The uncertaintly of the estimate is captured by the likelihood function (and we'll get back to this next week, in the Bayesian contect), but we'd like to quantify it with a few numbers.\n",
    "\n",
    "We *define* the uncertainty on our MLEs as second (partial) derivatives of log-likelihood:\n",
    "\n",
    "$$\\sigma_{jk} = \\left( - \\frac{d^2}{d\\theta_j} \\frac{\\ln L}{d\\theta_k} \\Biggr\\rvert_{\\theta=\\hat \\theta}\\right)^{-1/2}.$$\n",
    "\n",
    "Taken together, these entries (more accurately, their squares) are know as the *covariance matrix*.\n",
    "\n",
    "The marginal error bars for each parameter, $\\theta_i$ are given by the diagonal elements, $\\sigma_{ii}$. These are the \"error bars\" that are typically quoted with each measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In our example, the uncertainly on the mean is \n",
    "$$\\sigma_{\\mu} = \\left( - \\frac{d^2\\ln L(\\mu)}{d\\mu^2}\\Biggr\\rvert_{\\hat \\mu}\\right)^{-1/2}$$\n",
    "\n",
    "We find\n",
    "$$\\frac{d^2\\ln L(\\mu)}{d\\mu^2}\\Biggr\\rvert_{\\hat \\mu} = - \\sum_{i=1}^N\\frac{1}{\\sigma^2} = -\\frac{N}{\\sigma^2},$$\n",
    "since, again, $\\sigma = {\\rm constant}$.  \n",
    "\n",
    "Then $$\\sigma_{\\mu} = \\frac{\\sigma}{\\sqrt{N}}.$$\n",
    "\n",
    "So, our estimator of $\\mu$ is $\\overline{x}\\pm\\frac{\\sigma}{\\sqrt{N}}$, which is a result that you should be familiar with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is $\\pm \\sigma$? Errors as Gaussian Approximations to the Likelihood Function\n",
    "\n",
    "The result for $\\sigma_{\\mu}$ has been derived by expanding $\\ln L$ in a Taylor series and retaining terms up to second order (essentially, $\\ln L$ is approximated by a parabola, or an ellipsoidal surface in multidimensional cases, around its maximum). If this expansion is exact (as is the case for a Gaussian error distribution), then we've completely captured the error information.\n",
    "\n",
    "In general, this is not the case and the likelihood surface can significantly deviate from a smooth elliptical surface. Furthermore, it often happens in practice that the likelihood surface is multimodal. It is always a good idea to visualize the likelihood surface when in doubt (see examples in §5.6 in the textbook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is $\\pm \\sigma$? Errors as Gaussian Approximations to the Likelihood Function\n",
    "\n",
    "The $(\\hat \\mu - \\sigma_\\mu, \\hat \\mu + \\sigma_\\mu)$ range gives us a **confidence interval**.\n",
    "\n",
    "In frequentist interptetation, if we repeated the same measurement a hundred times, we'd find for 68 experiments the true value was within their computed confidence intervals ($1 \\sigma$ errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MLE applied to a Heteroscedastic Gaussian\n",
    "\n",
    "Now let's look a case where the errors are heteroscedastic.  For example if we are measuring the length of a rod and have $N$ measurements, $\\{x_i\\}$, where the error for each measurement, $\\sigma_i$ is known.  Since $\\sigma$ is not a constant, then following the above, we have\n",
    "\n",
    "$$\\ln L = {\\rm constant} - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma_i^2}.$$\n",
    "\n",
    "Taking the derivative:\n",
    "$$\\frac{d\\;{\\rm lnL}(\\mu)}{d\\mu}\\Biggr\\rvert_{\\hat \\mu} = \\sum_{i=1}^N \\frac{(x_i - \\hat \\mu)}{\\sigma_i^2} = 0,$$\n",
    "then simplifying:\n",
    "\n",
    "$$\\sum_{i=1}^N \\frac{x_i}{\\sigma_i^2} = \\sum_{i=1}^N \\frac{\\hat \\mu}{\\sigma_i^2},$$\n",
    "\n",
    "yields a MLE solution of \n",
    "$$\\hat \\mu = \\frac{\\sum_i^N (x_i/\\sigma_i^2)}{\\sum_i^N (1/\\sigma_i^2)},$$\n",
    "\n",
    "with uncertainty\n",
    "$$\\sigma_{\\mu} = \\left( \\sum_{i=1}^N \\frac{1}{\\sigma_i^2}\\right)^{-1/2}.$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goodness of Fit\n",
    "\n",
    "The MLE approach tells us what the \"best\" model parameters are, but not how good the fit actually is.  If the model is wrong, \"best\" might not be particularly revealing!  For example, if you have $N$ points drawn from a linear distribution, you can always fit the data perfectly with an $N-1$ order polynomial.  But that won't necessarily perfectly predict future measurements.\n",
    "\n",
    "We can describe the **goodness of fit** in words simply as whether or not it is likely to have obtained $\\ln L_0$ by randomly drawing from the data.  That means that we need to know the *distribution* of $\\ln L$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goodness of Fit\n",
    "\n",
    "For the Gaussian case we have just described, we can write\n",
    "\n",
    "$$z_i = (x_i-\\mu)/\\sigma,$$ then\n",
    "$$\\ln L = {\\rm constant} - \\frac{1}{2}\\sum_{i=1}^N z^2 = {\\rm constant} - \\frac{1}{2}\\chi^2.$$\n",
    "\n",
    "Here, $\\chi^2$ is the same thing that you may already be familar with and whose distribution we discussed last week. So **$\\ln L$ is distributed as $\\chi^2$ (with $N-k$ degrees of freedom).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goodness of Fit\n",
    "\n",
    "What does that tell us?\n",
    "\n",
    "The expectation value for the $\\chi^2$ distribution is $N − k$ and its standard deviation is $\\sqrt{2(N − k)}$. We typically have $N \\gg k$ (where $N$ is the number of data points, and $k$ is the number of parameters in the model). When that holds, it becomes useful to define **$\\chi^2$ per degree of freedom, $\\chi^2_{dof}$**, as:\n",
    "\n",
    "$$\\chi^2_{dof} = \\frac{1}{N-k}\\sum_{i=1}^N z^2_i.$$\n",
    "\n",
    "Therefore, for a good fit we would expect that $\\chi^2_{dof}\\approx 1$ (the expectation value).  If $\\chi^2_{dof}$ is significantly larger than 1 (how much larger?), then it is likely that we are not using the correct model.\n",
    "\n",
    "We can also get overly high or low values of $\\chi^2_{dof}$ if our errors are under- or over-estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Least Squares as Maximum Likelihood\n",
    "\n",
    "![alt text](figures/multiple-linear-regression-11-638.jpg)\n",
    "\n",
    "Credit: https://www.slideshare.net/jtneill/multiple-linear-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Statement of the problem:\n",
    "\n",
    "* We've collected a set of $N$ data points $x_i, y_i$.\n",
    "* Our model for their behavior is $y(x) = a x + b$, where $a$ and $b$ are unknowns.\n",
    "* We wish to find the \"best\" $a$ and $b$ that describe the measured data\n",
    "\n",
    "Solution:\n",
    "* What does \"good\" (or \"best\") mean? Intuitively, a good fit is one where the line \"passes through the points\" (or comes as close as possible to them). In other words, where the *residuals*, $r_i = y_i - y(x_i)$, are small.\n",
    "* Hmm, but minimizing the residual for one point may be in tension with the residuals of other points. How do we find the right balance?\n",
    "* Least squares wisdom: the path to balance lies in minimizing the ***sum of the squares of the residuals***\n",
    "\n",
    "$$ {min}_{a, b} \\, Q(a, b) = {min}_{a, b} \\sum_{i=1}^N (y_i - y(x_i))^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding the minimum\n",
    "\n",
    "$$ \\frac{\\partial Q}{\\partial a} = 0; \\;\\;{\\rm solve\\,for\\,a} $$\n",
    "$$ \\frac{\\partial Q}{\\partial b} = 0; \\;\\;{\\rm solve\\,for\\,b} $$\n",
    "\n",
    "Let's start solving:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Q}{\\partial a} = \\sum_{i=1}^N \\frac{\\partial}{\\partial a}(y_i - (a x_i + b))^2 = \n",
    "\\sum_{i=1}^N (y_i - (a x_i + b)) (- x_i) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Q}{\\partial b} = \\sum_{i=1}^N \\frac{\\partial}{\\partial b}(y_i - (a x_i + b))^2 = \n",
    "\\sum_{i=1}^N (y_i - (a x_i + b)) (- 1) = 0\n",
    "$$\n",
    "\n",
    "This gives us two coupled linear equations for $a, b$. After some straightforward but tedious math, we recover the well known result:\n",
    "\n",
    "$$ a = \\frac{\\sum_{i=1}^N (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^N (x_i - \\bar{x})^2} $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ b = \\bar{y} - a \\bar{x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problems\n",
    "\n",
    "Least squares method, as introduced above, has some problems:\n",
    "* Why east *squares*? Why not ${residuals}^4$? Or least absolute value? It's not clear that the choice of squares is well motivated.\n",
    "* What happens when we have error bars known for every datum $y_i$? We know the prescriptive answer is to divide the residual by the error, but where does that come from?\n",
    "* And what would we do if we had error bars specified for both $x$ *and* $y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting a Line using a Maximum Likelihood Estimator\n",
    "\n",
    "We can motivate the least squares method from ML perspective.\n",
    "\n",
    "Assume the scatter in our measurements (the residuals) is generated by a gaussian process. I.e.:\n",
    "\n",
    "$$ y_i = a x_i + b + r_i $$\n",
    "\n",
    "where $r_i$ is drawn from $N(0, \\sigma)$. Here, $\\sigma$ is the error the measurement induces.\n",
    "\n",
    "Let us compute the likelihood. First, we ask ourselves what is the probability $p(y_i|x_i, M(a, b), \\sigma)$ that a particular point $y_i$ would be measured. It is just the normal distribution:\n",
    "\n",
    "$$ p(y_i|x_i, M(a, b), \\sigma) = N(y_i - y(x)|\\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( - \\frac{(y_i - y(x_i))^2}{2 \\sigma^2} \\right) $$.\n",
    "\n",
    "Assuming the measurements are independent, the likelihood is then the product of individual probabilities for every $y_i$:\n",
    "\n",
    "$$ L(a, b) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( - \\frac{(y_i - y(x_i))^2}{2 \\sigma^2} \\right) $$.\n",
    "\n",
    "and $\\ln L$ is equal to:\n",
    "\n",
    "$$ \\ln L(a, b) = constant - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^N (y_i - y(x_i))^2 $$.\n",
    "\n",
    "This is the expression that we now minimize with respect to $a$ and $b$ to find ML estimators for those parameters. But it's also the same expression we've derived using the least-squares prescription! (up to a constant and a factor that don't change anything.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Least squares as a Maximum Likelihood estimator\n",
    "\n",
    "This shows that LSQ is an MLE of a model with gaussian errors. It gives us a better understanding of its applicability and properties (i.e., all the nice properties of ML estimators apply). Now it's also straightforward to see how to generalize it to the heteroscedastic case.\n",
    "\n",
    "More generally, note that I didn't have to use the fact we were using LSQ to fit a line to show that it's an ML estimator (i.e., I just specified $y(x_i)$; this function could have been anything). Therefore, more broadly, for an arbitrary model y_i(x) whose errors are gaussian, it will be the case that log-likelihood is:\n",
    "\n",
    "$$ \\ln L(a, b) = constant - \\sum_{i=1}^N \\frac{(y_i - M(x_i; \\theta))^2}{2 \\sigma^2} $$.\n",
    "\n",
    "where $\\theta$ are its parameters (one or more). We'll use this soon to compute an ML estimator for the flux of a star."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Learning goals for Week 2 (mostly based on Chapter 3 material): \n",
    "\n",
    "- Probability Rules (notation, definitions, conditional probability, Bayes Rule).\n",
    "- How do I robustly estimate location and scale parameters of a one-dimensional data set?  \n",
    "- Statistical distributions and how do we describe them? \n",
    "- Estimators, location and scale, sample vs. population, bias and scatter.\n",
    "\n",
    "\n",
    "- How do I use python to generate various statistical distributions, such as Cauchy, Laplace, etc.  \n",
    "- The Central Limit Theorem. \n",
    "- Robust estimators. \n",
    "- How do I robustly estimate parameters of a two-dimensional Gaussian?  \n",
    "- Bivariate and Multivariate Distribution Functions.  \n",
    "- How do we make a histogram and why? How do we choose optimal bin width?\n",
    "  Do histogram bins need to be same size?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Notation\n",
    "\n",
    "### Variables\n",
    "\n",
    "First we need to go over some of the notation that the book uses.   \n",
    "\n",
    "$x$ is a scalar quantity, measured $N$ times\n",
    "\n",
    "$x_i$ is a single measurement with $i=1,...,N$\n",
    "\n",
    "$\\{x_i\\}$ refers to the set of all N measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Population vs Empirical PDF\n",
    "\n",
    "We are generally trying to *estimate* $h(x)$, the *true* distribution from which the values of $x$ are drawn. We will refer to $h(x)$ as the probability density (distribution) function or the \"pdf\" and $h(x)dx$ is the propobability of a value lying between $x$ and $x+dx$. \n",
    "\n",
    "While $h(x)$ is the \"true\" pdf (or **population** pdf).  What we *measure* from the data is the **empirical** pdf, which is denoted $f(x)$.  So, $f(x)$ is a *model* of $h(x)$.  In principle, with infinite data $f(x) \\rightarrow h(x)$, but in reality measurement errors keep this from being strictly true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parametric vs. Non-parametric\n",
    "\n",
    "If we are attempting to guess a *model* for $h(x)$, then the process is *parametric*.  With a model solution we can generate new data that should mimic what we measure.\n",
    "\n",
    "If we are not attempting to guess a model, then the process is *nonparametic*.  That is we are just trying to describe the data that we see in the most compact manner that we can, but we are not trying to produce mock data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probability\n",
    "\n",
    "The probability of $A$, $p(A)$, is the probability that some event will happen (say a coin toss), or if the process is continuous, the probability of $A$ falling in a certain range.  (N.B., Technically these two things are different and sometimes are indicated by $P$ and $p$, but we'll ignore that here).\n",
    "\n",
    "$p(A)$ must be positive definite for all $A$ and the sum/integral of the pdf must be unity.\n",
    "\n",
    "If we have two events, $A$ and $B$, the possible combinations are illustrated by the following figure:\n",
    "![Figure 3.1](http://www.astroml.org/_images/fig_prob_sum_1.png)\n",
    "\n",
    "$A \\cup B$ is the *union* of sets $A$ and $B$.\n",
    "\n",
    "$A \\cap B$ is the *intersection* of sets $A$ and $B$.\n",
    "\n",
    "The probability that *either* $A$ or $B$ will happen (which could include both) is the *union*, given by\n",
    "\n",
    "$$p(A \\cup B) = p(A) + p(B) - p(A \\cap B)$$\n",
    "\n",
    "The figure makes it clear why the last term is necessary.  Since $A$ and $B$ overlap, we are double-counting the region where *both* $A$ and $B$ happen, so we have to subtract this out.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability that *both* $A$ and $B$ will happen, $p(A \\cap B)$, is \n",
    "\n",
    "$$p(A \\cap B) = p(A|B)p(B) = p(B|A)p(A)$$\n",
    "\n",
    "where p(A|B) is the probability of A *given that* B is true and is called the *conditional probability*.  So the $|$ is short for \"given that\".\n",
    "\n",
    "In other words: *\"The probability that both A and B have occured is equal to the probability B has occured times the probability that A will occur if B occured\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If events B_i are disjoint and their union is the set of all possible outcomes, then the **law of total probability** says that:\n",
    "\n",
    "$$p(A) = \\sum_ip(A|B_i)p(B_i)$$\n",
    "\n",
    "Example:\n",
    "\n",
    "    A = hit head on door frame, B = { is tall, is average, is short }\n",
    "    P(A) = P(A|is tall) + P(B|is average) + P(C|is short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "N.B.  Just to be annoying, different people use different notation and the following all mean the same thing\n",
    "\n",
    "$$p(A \\cap B) = p(A,B) = p(AB) = p(A \\,{\\rm and}\\, B)$$\n",
    "\n",
    "We will use the comma notation as in the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is important to realize that the following is *always* true\n",
    "$$p(A,B) = p(A|B)p(B) = p(B|A)p(A)$$\n",
    "\n",
    "However, if $A$ and $B$ are ***independent***, then \n",
    "\n",
    "$$p(A,B) = p(A)p(B)$$\n",
    "\n",
    "Example:\n",
    "\n",
    "     John is successful and John is a Libra.\n",
    "     \n",
    "In other words, ***knowing A happened tells us nothing about whether B happened (or will happen), and vice versa***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at another example.\n",
    "\n",
    "If you have a bag with 5 marbles, 3 yellow and 2 blue and you want to know the probability of picking 2 yellow marbles in a row, that would be\n",
    "\n",
    "$$p(Y_1,Y_2) = p(Y_1)p(Y_2|Y_1).$$\n",
    "\n",
    "$p(Y_1) = \\frac{3}{5}$ since you have an equally likely chance of drawing any of the 5 marbles.\n",
    "\n",
    "If you did not put the first marble back in the back after drawing it (sampling *without* \"replacement\"), then the probability\n",
    "$p(Y_2|Y_1) = \\frac{2}{4}$, so that\n",
    "$$p(Y_1,Y_2) = \\frac{3}{5}\\frac{2}{4} = \\frac{3}{10}.$$\n",
    "\n",
    "But if you put the first marble back, then\n",
    "$p(Y_2|Y_1) = \\frac{3}{5} = p(Y_2)$, so that \n",
    "$$p(Y_1,Y_2) = \\frac{3}{5}\\frac{3}{5} = \\frac{9}{25}.$$\n",
    "\n",
    "In the first case $A$ and $B$ (or rather $Y_1$ and $Y_2$) are *not* independent, whereas in the second case they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a more complicated example from \n",
    "[Jo Bovy's class at UToronto](http://astro.utoronto.ca/%7Ebovy/teaching.html)\n",
    "![Bovy_L1-StatMiniCourse_page21](figures/Bovy_L1-StatMiniCourse_page21.png)\n",
    "\n",
    "As illustrated, \n",
    "$$p(A \\,{\\rm or}\\, B|C) = p(A|C) + p(B|C) - p(A \\, {\\rm and}\\, B|C)$$ \n",
    "\n",
    "This illustration also explains why $$p(x|y)p(y) = p(y|x)p(x)$$ (used below),\n",
    "or in the notation of this figure: $$p(A \\, {\\rm and}\\, B) \\equiv p(A,B) = p(A|B)p(B) = p(B|A)p(A)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Need more help with this?  Try watching some Khan Academy videos and working through the exercises:\n",
    "[https://www.khanacademy.org/math/probability/probability-geometry](https://www.khanacademy.org/math/probability/probability-geometry)\n",
    "\n",
    "[https://www.khanacademy.org/math/precalculus/prob-comb](https://www.khanacademy.org/math/precalculus/prob-comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayes' Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We've seen that the probability of $x$ and $y$ occurring can be written as:\n",
    "\n",
    "$$p(x,y) = p(x|y)p(y) = p(y|x)p(x)$$\n",
    "\n",
    ". We can define the ***marginal probability*** as\n",
    "\n",
    "$$p(x) = \\int p(x,y)dy,$$\n",
    "\n",
    "where marginal means the probability of $x$ occurring irrespective what $y$ is. This is essentially projecting on to one axis (integrating over the other axis, see the figure in the Notebook, below).\n",
    "\n",
    "Given these two, we can write:\n",
    "\n",
    "$$p(x) = \\int p(x|y)p(y) dy$$\n",
    "\n",
    "This is just the law of total probability, but for continous variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Marginal and contitional probability distributions\n",
    "\n",
    "In the following figure, we have a 2-D distribution in $x-y$ parameter space.  Here $x$ and $y$ are *not* independent as, once you pick a $y$, your values of $x$ are constrained.\n",
    "\n",
    "The *marginal* distributions are shown on the left and bottom sides of the left panel.  As the equation above says, this is just the integral along the $x$ direction for a given $y$ (left side panel) or the integral along the $y$ direction for a given $x$ (bottom panel).  \n",
    "\n",
    "The three panels on the right show the *conditional* probability (of $x$) for three $y$ values: $$p(x|y=y_0)$$  These are just \"slices\" through the 2-D distribution.\n",
    "\n",
    "![http://www.astroml.org/_images/fig_conditional_probability_1.png](http://www.astroml.org/_images/fig_conditional_probability_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then, again starting with:\n",
    "\n",
    "$$p(x|y)p(y) = p(y|x)p(x)$$\n",
    "\n",
    "we can write:\n",
    "\n",
    "$$p(y|x) = \\frac{p(x|y)p(y)}{p(x)} = \\frac{p(x|y)p(y)}{\\int p(x|y)p(y) dy}$$\n",
    "\n",
    "which in words says that\n",
    "\n",
    "> the (conditional) probability of $y$ given $x$ is just the (conditional) probability of $x$ given $y$ times the (marginal) probability of $y$ divided by the (marginal) probability of $x$, where the latter is just the integral of the numerator.\n",
    "\n",
    "This is **Bayes' rule**, which itself is not at all controversial, though its application can be as we'll discuss later (Week 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "![TextbookGrab1](figures/grab1.jpg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Descriptive statistics\n",
    "\n",
    "Typically, we collect some *samples* (series of measurements, or catalogs of stars) that can be thought of as being drawn from som underlying distribution (e.g., distribution of errors, or the distribution of stars in the Milky Way).\n",
    "\n",
    "We often don't care much about the individual samples, other than to use them ***to learn more about the underlying distributions and their properties (e.g., the mean (location), width (scale), etc.)***. How do we do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![SlideGrab](figures/p5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Normal probability density function (pdf): $$p(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\right).$$\n",
    "\n",
    "Cumulative distribution function (cdf): $$\\Phi(x|\\mu,\\sigma) = \\int_{-\\infty}^{x}  p(x'|\\mu,\\sigma) dx' $$\n",
    "$$\\Phi(\\infty|\\mu,\\sigma) = 1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian confidence levels\n",
    "\n",
    "The probability of a measurement drawn from a Gaussian distribution that is between $\\mu-a$ and $\\mu+b$ is\n",
    "$$\\int_{\\mu-a}^{\\mu+b} p(x|\\mu,\\sigma) dx.$$\n",
    "For $a=b=1\\sigma$, we get the familar result of 68.3%.  For $a=b=2\\sigma$ it is 95.4%.  So we refer to the range $\\mu \\pm 1\\sigma$ and $\\mu \\pm 2\\sigma$ as the 68% and 95% **confidence limits**, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## now let's go back to the problem of estimating location and scale\n",
    "## given a sample, such as gaussSample above, how do we estimate its mu and sigma?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![SlideGrab](figures/p4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![SlideGrab](figures/p3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sample vs. Population Statistics \n",
    "\n",
    "Statistics estimated from the *data* are called _sample statistics_ as compared to _population statistics_ which come from knowing the functional form of the pdf. For example, the expectation value for a known h(x) is\n",
    "\n",
    "$$\\mu \\equiv E(x) = \\int_{-\\infty}^{\\infty} x h(x) dx,$$\n",
    "\n",
    "where $h(x)$ must be properly normalized (the integral gets replaced by a sum for discrete distributions).\n",
    "\n",
    "E(x) is the expecation value of $x$.  If you want the expectation value of something else--say $x^2$ or $(x-\\mu)^2$, you replace $x$ with that. Importantly, the *variance* is the expectation value of $(x-\\mu)^2$\n",
    "\n",
    "$$\\sigma^2 \\equiv V = \\int_{-\\infty}^{\\infty}  (x-\\mu)^2 h(x) dx,$$\n",
    "\n",
    "where, again, the integral gets replaced by a sum for discrete distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Specifically, $\\mu$ is the *population average*, i.e., it is the expecation value of $x$ for $h(x)$.  But we don't *know* $h(x)$! So we do the next best thing, and estimate it from the data:\n",
    "\n",
    "$$ \\hat{h}(x) = \\sum_{i=1}^N \\frac{\\delta_(x - x_i)}{N}$$\n",
    "\n",
    "Plugging into the previous equations, we derive the **sample mean**, $\\overline{x}$ as an *estimator* of $\\mu$ and defined as\n",
    "$$\\overline{x} \\equiv \\frac{1}{N}\\sum_{i=1}^N x_i,$$\n",
    "which we determine from the data itself. We'll hear more about estimators in Week 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Similarly, the **sample variance** ($s^2$, where \n",
    "$s$ is the sample standard deviation) is an *estimator* of $\\sigma^2$:\n",
    "$$s^2 \\equiv \\frac{1}{N-1}\\sum_{i=1}^N (x_i-\\overline{x})^2.$$\n",
    "\n",
    "**WAIT!!!** Why do we have (N-1) and not N (as in expression for the mean)???\n",
    "\n",
    "The reason for the (N-1) term instead of the naively expected N in the second expression is related to the fact that $\\overline{x}$ is also determined from data (we will discuss this subtle fact and the underlying statistical justification for the (N-1) term in more detail in Week 4 lectures. With N replaced by (N-1) (the so-called Bessel’s correction), the sample variance (i.e., $\\sigma^2$) becomes unbiased (and the sample standard deviation becomes a less biased, but on average still underestimated, estimator of the true standard deviation). \n",
    "\n",
    "What does \"biased\" mean? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![SlideGrab](figures/p13.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![SlideGrab](figures/p10.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![SlideGrab](figures/p11.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![SlideGrab](figures/p12.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![SlideGrab](figures/p8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Anscombe's quartet comprises four datasets that have nearly identical simple descriptive statistics, yet appear very different when graphed. \n",
    "\n",
    "![SlideGrab](figures/AnscombeQuartet.jpg)\n",
    "\n",
    "![SlideGrab](figures/AnscombeQuartetTable.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Uncertainty for the mean and the sample standard deviation\n",
    "\n",
    "We introduced the uncertainty of our estimates of $\\overline{x}$ and $s$: \n",
    "\n",
    "$$ \\sigma_{\\overline{x}} = \\frac{s}{\\sqrt{N}},$$\n",
    "\n",
    "which we call the *standard error of the mean*, and the uncertainty of $s$:\n",
    "\n",
    "$$\\sigma_s = \\frac{s}{\\sqrt{2(N-1)}} = \\frac{1}{\\sqrt{2}}\\sqrt{\\frac{N}{N-1}}\\sigma_{\\overline{x}}.$$\n",
    "\n",
    "Note that for large $N$, $\\sigma_{\\overline{x}} \\sim \\sqrt{2}\\sigma_s$ and for small $N$, $\\sigma_s$ is not much smaller than $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Standard Error vs. Standard Deviation\n",
    "\n",
    "Note the difference between the ***standard deviation*** and and the ***standard error***. The former describes the property of the distribution we're attempting to estimate (i.e., how far is a typical individual sample away from the sample mean); the latter describes the precision of our estimate of some quantity (e.g., the mean itself).\n",
    "\n",
    "Example:\n",
    "* An average human is 1.65m (female) and 1.78 (male) tall. With enough measurements, the error of that estimate of the mean can get almost arbitrarily small.\n",
    "* Human height is approximately normally distributed, with a standard deviation of $9-10$ cm. No matter how many measurements we perform, the that standard deviation will not reduce -- it's the property of the population (and sample).\n",
    "\n",
    "Fun fact: *\"The people of the Dinaric Alps mainly South Slavs (Montenegro and East Herzegovina [and Croatia]) are on record as being the tallest in the world, with a male average height of 185.6 cm (6 ft 1.1 in) and female average height of 170.9 cm (5 ft 7.3 in).\"* -- Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing the standard error\n",
    "\n",
    "Let's see how it looks in practice by doing a few numerical experiments: we'll:\n",
    "\n",
    "* draw $k=10$ numbers from $N(\\mu=1.0, \\sigma=0.1)$ and compute their mean\n",
    "* repeat this computation M=10,000 times and plot the distribution of these M means\n",
    "\n",
    "Will this distribution well described by $N(1.0, \\frac{0.1}{\\sqrt{k}})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![SlideGrab](figures/p6.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice (online) proof of the CLT: http://www.cs.toronto.edu/~yuvalf/CLT.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![SlideGrab](figures/p7.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Robust Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![SlideGrab](figures/p22.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While it is perhaps most common to compute the mean, the median is a more *robust* estimator of the (true) mean location of the distribution.  That's because it is less affected by outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![SlideGrab](figures/p24.jpg)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mode\n",
    "\n",
    "The mode is the most probable value, determined from the peak of the distribution, which is the value where the derivative is 0:\n",
    "\n",
    "$$ \\left(\\frac{dh(x)}{dx}\\right)_{x_m} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The mode can be estimated (at least for a Gaussian distribution) as\n",
    "\n",
    "$$x_m = 3q_{50} - 2\\mu$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Uniform Distribution\n",
    "\n",
    "The uniform distribution is perhaps more commonly called a \"top-hat\" or a \"box\" distribution.  It is specified by a mean, $\\mu$, and a width, $W$, where\n",
    "\n",
    "$$p(x|\\mu,W) = \\frac{1}{W}$$\n",
    "\n",
    "over the range $|x-\\mu|\\le \\frac{W}{2}$ and $0$ otherwise.  That says that \"given $\\mu$ AND $W$, the probability of $x$ is $\\frac{1}{W}$\" (as long as we are within a certain range).\n",
    "\n",
    "Since we are used to thinking of a Gaussian as the *only* type of distribution the concept of $\\sigma$ (aside from the width) may seem strange.  But $\\sigma$ as mathematically defined above applies here and\n",
    "$$\\sigma = \\frac{W}{\\sqrt{12}}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Log Normal\n",
    "\n",
    "Note that if $x$ is Gaussian distributed with $\\mathscr{N}(\\mu,\\sigma)$, then $y=\\exp(x)$ will have a **log-normal** distribution, where the mean of y is $\\exp(\\mu + \\sigma^2/2)$.  Try it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\chi^2$ Distribution\n",
    "\n",
    "We'll run into the $\\chi^2$ distribution when we talk about Maximum Likelihood in the next chapter.\n",
    "\n",
    "If we have a Gaussian distribution with values ${x_i}$ and we scale and normalize them according to\n",
    "\n",
    "$$z_i = \\frac{x_i-\\mu}{\\sigma},$$\n",
    "\n",
    "then the sum of squares, $Q$ \n",
    "\n",
    "$$Q = \\sum_{i=1}^N z_i^2,$$\n",
    "\n",
    "will follow the *$\\chi^2$ distribution with $k$ degrees of freedom* (see next slide for discussion of $k$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The *number of degrees of freedom*, $k$ is given by the number of data points, $N$ (minus any constraints).  The pdf of $Q$ given $k$ defines the $\\chi^2$ distribution and is given by\n",
    "\n",
    "$$p(Q|k)\\equiv \\chi^2(Q|k) = \\frac{1}{2^{k/2}\\Gamma(k/2)}Q^{k/2-1}\\exp(-Q/2),$$\n",
    "\n",
    "where $Q>0$ and the $\\Gamma$ function would just be the usual factorial function if we were dealing with integers, but here we have half integers.\n",
    "\n",
    "This is ugly, but it is really just a formula like anything else.  Note that the shape of the distribution *only* depends on the sample size $N=k$ and not on $\\mu$ or $\\sigma$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-squared per degree of freedom\n",
    "\n",
    "For large $k$ (say, $k > 10$ or so), $\\chi^2$-distribution becomes well approximated by the Normal distribution (Gaussian):\n",
    "\n",
    "$$ p(\\chi^2|k) \\sim N(\\chi^2 | k, \\sqrt{2k}) $$\n",
    "\n",
    "In practice we frequently divide $\\chi^2$ by the number of degrees of freedom, and work with:\n",
    "\n",
    "$$\\chi^2_{dof} = \\frac{1}{N-1} \\sum_{i=1}^N \\left(\\frac{x_i-\\overline{x}}{\\sigma}\\right)^2$$\n",
    "\n",
    "which is distributed as\n",
    "\n",
    "$$ p(\\chi^2_{dof}) \\sim N\\left(\\chi^2_{dof} \\rvert 1, \\sqrt{\\frac{2}{N-1}}\\right) $$\n",
    "\n",
    "(where $k = N-1$, and $N$ is the number of samples). Therefore, we expect $\\chi^2_{dof}$ to be 1, to within a few $\\sqrt{\\frac{2}{N-1}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![SlideGrab](figures/p20.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![SlideGrab](figures/p25.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multivariate distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate and Multivariate Distribution Functions\n",
    "\n",
    "Up to now we have been dealing with one-dimensional distribution functions.  Let's now consider a two dimensional distribution $h(x,y)$ where $$\\int_{-\\infty}^{\\infty}dx\\int_{-\\infty}^{\\infty}h(x,y)dy = 1.$$  $h(x,y)$ is telling us the probability that $x$ is between $x$ and $dx$ and *also* that $y$ is between $y$ and $dy$.\n",
    "\n",
    "Then we have the following definitions:\n",
    "\n",
    "$$\\sigma^2_x = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(x-\\mu_x)^2 h(x,y) dx dy$$\n",
    "\n",
    "$$\\sigma^2_y = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(y-\\mu_y)^2 h(x,y) dx dy$$\n",
    "\n",
    "$$\\mu_x = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}x h(x,y) dx dy$$\n",
    "\n",
    "$$\\sigma_{xy} = Cov(x,y) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(x-\\mu_x) (y-\\mu_y) h(x,y) dx dy$$\n",
    "\n",
    "If $x$ and $y$ are uncorrelated, then we can treat the system as two independent 1-D distributions.  This means that choosing a range on one variable has no effect on the distribution of the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can write a 2-D Gaussian pdf as\n",
    "$$p(x,y|\\mu_x,\\mu_y,\\sigma_x,\\sigma_y,\\sigma_{xy}) = \\frac{1}{2\\pi \\sigma_x \\sigma_y \\sqrt{1-\\rho^2}} \\exp\\left(\\frac{-z^2}{2(1-\\rho^2)}\\right),$$\n",
    "\n",
    "where $$z^2 = \\frac{(x-\\mu_x)^2}{\\sigma_x^2} + \\frac{(y-\\mu_y)^2}{\\sigma_y^2} - 2\\rho\\frac{(x-\\mu_x)(y-\\mu_y)}{\\sigma_x\\sigma_y},$$\n",
    "\n",
    "with $$\\rho = \\frac{\\sigma_{xy}}{\\sigma_x\\sigma_y}$$\n",
    "as the (dimensionless) correlation coefficient.\n",
    "\n",
    "If $x$ and $y$ are perfectly correlated then $\\rho=\\pm1$ and if they are uncorrelated, then $\\rho=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The pdf is now not a histogram, but rather a series of contours in the $x-y$ plane.   These are centered at $(x=\\mu_x, y=\\mu_y)$ and are tilted at angle $\\alpha$, which is given by\n",
    "$$\\tan(2 \\alpha) = 2\\rho\\frac{\\sigma_x\\sigma_y}{\\sigma_x^2-\\sigma_y^2} = 2\\frac{\\sigma_{xy}}{\\sigma_x^2-\\sigma_y^2}.$$\n",
    "\n",
    "For example (Figure 3.22 from the textbook):\n",
    "![Figure 3.22](http://www.astroml.org/_images/fig_bivariate_gaussian_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can define new coordinate axes that are aligned with the minimum and maximum widths of the distribution.  These are called the **principal axes** and are given by\n",
    "$$P_1 = (x-\\mu_x)\\cos\\alpha + (y-\\mu_y)\\sin\\alpha,$$\n",
    "and\n",
    "$$P_2 = -(x-\\mu_x)\\sin\\alpha + (y-\\mu_y)\\cos\\alpha.$$\n",
    "\n",
    "The widths in this coordinate system are\n",
    "$$\\sigma^2_{1,2} = \\frac{\\sigma_x^2+\\sigma_y^2}{2}\\pm\\sqrt{\\left(\\frac{\\sigma_x^2-\\sigma_y^2}{2}\\right)^2 + \\sigma^2_{xy}}.$$\n",
    "\n",
    "Note that the correlation vanishes in this coordinate system and the bivariate Gaussian is just a product of two univariate Gaussians.  This concept will be crucial for understanding Principal Component Analysis when we get to Chapter 7, where PCA extends this idea to even more dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the univariate case we used $\\overline{x}$ and $s$ to *estimate* $\\mu$ and $\\sigma$.  In the bivariate case we estimate 5 parameters: $(\\overline{x},\\overline{y},s_x,s_y,s_{xy})$.  \n",
    "\n",
    "As with the univariate case, it is important to realize that outliers can bias these estimates and that it may be more appropriate to use the median rather than the mean as a more robust estimator for $\\mu_x$ and $\\mu_y$.  Similarly we want robust estimators for the other parameters of the fit.  We won't go into that in detail right now, but see Figure 3.23 from the textbook for an example:\n",
    "\n",
    "![Ivezic, Figure 3.23](http://www.astroml.org/_images/fig_robust_pca_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
